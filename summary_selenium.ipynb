{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Website URL Summarizer (Selenium)\n",
    "\n",
    "Uses a real browser via Selenium to fetch webpages that block simple HTTP requests, then summarizes using OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_TAGS = [\n",
    "    \"script\", \"style\", \"nav\", \"footer\", \"header\",\n",
    "    \"aside\", \"form\", \"noscript\", \"iframe\",\n",
    "]\n",
    "MAX_TEXT_LENGTH = 15_000\n",
    "\n",
    "\n",
    "def extract_text(url):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            lambda d: d.execute_script(\"return document.readyState\") == \"complete\"\n",
    "        )\n",
    "        html = driver.page_source\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for tag in soup(REMOVE_TAGS):\n",
    "        tag.decompose()\n",
    "\n",
    "    text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    return text[:MAX_TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY not set in .env file.\")\n",
    "\n",
    "    model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.3,\n",
    "        messages=[\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a concise and accurate web content summarizer. \"\n",
    "            \"Your task is to extract and summarize the main content of a webpage. \"\n",
    "            \"Rules:\\n\"\n",
    "            \"- Produce a clear summary in 3-5 paragraphs.\\n\"\n",
    "            \"- Focus on the key points, facts, and arguments.\\n\"\n",
    "            \"- Preserve important names, dates, and figures.\\n\"\n",
    "            \"- Ignore boilerplate such as cookie notices, navigation menus, \"\n",
    "            \"ads, footers, and sidebar content.\\n\"\n",
    "            \"- If the content is too short or mostly boilerplate, \"\n",
    "            \"state that no meaningful content was found.\"\n",
    "        ),\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"Summarize this webpage content:\\n\\n{text}\"},\n",
    "],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "url = \"https://github.com/faridraisi/website-summary\""
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching https://github.com/faridraisi/website-summary ...\n",
      "Extracted 3090 characters.\n"
     ]
    }
   ],
   "source": [
    "text = None\n",
    "\n",
    "print(f\"Fetching {url} ...\")\n",
    "try:\n",
    "    text = extract_text(url)\n",
    "    print(f\"Extracted {len(text)} characters.\")\n",
    "except TimeoutException:\n",
    "    print(\"Page took too long to load.\")\n",
    "except WebDriverException as e:\n",
    "    msg = str(e).split(\"\\n\")[0]\n",
    "    print(f\"Browser error: {msg}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing...\n",
      "\n",
      "The GitHub repository \"faridraisi/website-summary\" hosts a Python tool designed to summarize web pages by extracting meaningful text and utilizing OpenAI's API. The project includes two Jupyter notebooks: `summary_requests.ipynb`, which employs the `requests` library for lightweight fetching of web pages, and `summary_selenium.ipynb`, which uses Selenium with a headless Chrome browser to handle JavaScript-heavy and bot-protected sites.\n",
      "\n",
      "To set up the tool, users need Python 3.11 or higher, the `uv` package, and Chrome for the Selenium notebook. The setup process involves cloning the repository, navigating to the project directory, and configuring an environment file with an OpenAI API key. Users can then open either notebook in VS Code or Jupyter, set the desired URL, and execute the cells to obtain a summary.\n",
      "\n",
      "The tool operates in four main steps: fetching the webpage (using either `requests` or Selenium), cleaning the content by removing non-essential tags, extracting the clean text (limited to 15,000 characters), and summarizing the text through the OpenAI API to generate a concise summary of 3-5 paragraphs. Key dependencies for the project include `requests` for HTTP requests, `beautifulsoup4` for HTML parsing, and `openai` for interacting with the OpenAI API.\n",
      "\n",
      "Overall, this repository provides a practical solution for users looking to summarize web content efficiently, catering to both standard and more complex web pages.\n"
     ]
    }
   ],
   "source": [
    "if not text or not text.strip():\n",
    "    print(\"No text to summarize. Check the fetch step above.\")\n",
    "else:\n",
    "    print(\"Summarizing...\")\n",
    "    try:\n",
    "        summary = summarize(text)\n",
    "        print(f\"\\n{summary}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}